{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slYJRGUGWi7M",
        "colab_type": "text"
      },
      "source": [
        "# CE-40719: Deep Learning\n",
        "## HW3 - CNN / CNN Case Studies / CNN Applications\n",
        "(23 points)\n",
        "\n",
        "#### Name: Seyed Shayan Nazemi\n",
        "#### Student No.: 98209037"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9th3celWi7O",
        "colab_type": "text"
      },
      "source": [
        "In this assignment we go through the following topics:\n",
        "- Writing custome pytorch modules\n",
        "- Using `tensorboard` for logging and visualization\n",
        "- Data Augmentation\n",
        "- Saving / Loading Models\n",
        "\n",
        "Please Keep in mind that:\n",
        "- You can not use out-of-the-box pytorch modules (nn.Conv2d, nn.Linear, nn.BatchNorm, nn.Dropout, ...)\n",
        "- You can run this notebook on your computer. If you prefer using Google Colab you may lose some of the functionalities of `tensorboard` (like Projector). You can install `tensorboard` on your computer using package manager of your choice, and download `runs` folder from Google Colab and run it locally using `tensorboard --logdir=runs`.\n",
        "- Use the [documentation](https://pytorch.org/docs/stable/index.html).\n",
        "\n",
        "In this assignment we are going to train a convolutional neural network to classify images from [fashion-mnist](https://github.com/zalandoresearch/fashion-mnist) dataset. Fashion-mnist is a simple dataset containing 60000 training and 10000 test $28 \\times 28$ grayscale images of 10 different classes. Each class corresponds to a different kind of clothing. \n",
        "\n",
        "## 1. Setup (1.5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YefZWKdV2ehA",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VT1aKupTz902",
        "outputId": "e7a7fb15-db75-4101-9e1d-4f2b8d31cf08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXRuw0bDZsft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2_TNe7oE3kI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "281caac6-76cd-458a-d514-e49ca15c90e7"
      },
      "source": [
        "# import os\n",
        "# logs_base_dir = \"runs\"\n",
        "# os.makedirs(logs_base_dir, exist_ok=True)\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 7655), started 0:31:04 ago. (Use '!kill 7655' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oHj3j8MWi7U",
        "colab_type": "text"
      },
      "source": [
        "To easily train your model on different gpu devices or your computer's cpu you can define a `torch.device` object corresponding to that device and use `.to(device)` method to easily move modules or tensors to different devices. Pytorch provides helper functions in [torch.cuda](https://pytorch.org/docs/stable/cuda.html) package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h2VI3dPWi7V",
        "colab_type": "code",
        "outputId": "8cff51ac-ba53-46c4-bce8-3c7f164fbcc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# create a cpu device if cuda is not available or cuda_device=None otherwise\n",
        "# create a cuda:{cuda_device} device.\n",
        "#################################################################################\n",
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = torch.cuda.current_device()\n",
        "    device = torch.device('cuda', cuda_device)\n",
        "else:\n",
        "    device = torch.device('cpu', 0)\n",
        "pass\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "print(device)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqxq0663Wi7X",
        "colab_type": "text"
      },
      "source": [
        "Fashion-mnist dataset is available in `torchvision.datasets` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gYm5KjpuQ0VX",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# Initialize and download trainset and testset with datasets.FashionMNIST and\n",
        "# transform data into torch.Tensor. Initialize trainloader and testloader with\n",
        "# given batch_size.\n",
        "#################################################################################\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "trainset = datasets.FashionMNIST(root='./data',\n",
        "                                    train=True,\n",
        "                                    download=True,\n",
        "                                    transform=transform)\n",
        "\n",
        "testset = datasets.FashionMNIST(root='./data',\n",
        "                                   train=False,\n",
        "                                   download=True,\n",
        "                                   transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n",
        "\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT_cx0bPWi7a",
        "colab_type": "text"
      },
      "source": [
        "To get a sense of data it is always helpfull to see a few of samples. We can do this using `tensorboard`. Please read [this](https://pytorch.org/docs/stable/tensorboard.html) documentation page to get familiar with tensorboard. Run the following cell to intialize a SummaryWriter and log some of the training images to tensorboard. You can run tensorboard using `tensorboard --logdir=runs` and view images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOaW3VX4INws",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter('./runs/FashionMNIST')\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "img_grid = utils.make_grid(images[:16], nrow=4)\n",
        "\n",
        "writer.add_image('FashionMNIST', img_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9hprG-YWi7e",
        "colab_type": "text"
      },
      "source": [
        "We can also visualize data (or any representation of it) using dimmensionality reduction techniques provided by tensorboard. The following cell adds raw pixel values as embeddings to visualize data. You can see visualizations in projector tab of tensorboard after running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bML1OI8kkmJv",
        "outputId": "0a67a313-9266-4ab6-f4eb-2629c7ecf7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "def select_n_random(data, labels, n=100):\n",
        "    perm = torch.randperm(len(data))\n",
        "    return data[perm][:n], labels[perm][:n]\n",
        "\n",
        "nimages, nlabels = select_n_random(trainset.data, trainset.targets)\n",
        "\n",
        "writer.add_embedding(nimages.view(-1, 28 * 28), \n",
        "                     metadata=[classes[label] for label in nlabels],\n",
        "                     label_img=nimages.unsqueeze(1), \n",
        "                     tag='raw_pixels')\n",
        "writer.flush()"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-223-bf05a710d6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                      \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                      \u001b[0mlabel_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      tag='raw_pixels')\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_embedding\u001b[0;34m(self, mat, metadata, label_img, global_step, tag, metadata_header)\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfXZrW89Wi7i",
        "colab_type": "text"
      },
      "source": [
        "## 2. Modules (7 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDSsz8SCWi7i",
        "colab_type": "text"
      },
      "source": [
        "In this part you will define all the required modules for a convolutional model. You can only use functional package `torch.nn.functional` unless stated otherwise.\n",
        "\n",
        "### 2.1 Convolution Module (1.5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lVXQl0hcScTT",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# define convolution parameters using nn.Parameter.\n",
        "# initialize weihgt using nn.init.kaiming_uniform and bias by zeroes\n",
        "# use F.conv2d in forward method.\n",
        "#################################################################################\n",
        "class Conv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
        "        super(Conv2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.W = nn.Parameter(nn.init.kaiming_uniform_(torch.empty(out_channels, in_channels, kernel_size[0], kernel_size[1])))\n",
        "        self.b = nn.Parameter(nn.init.zeros_(torch.empty(out_channels)))\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n",
        "        return out\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUuOnM3Wi7l",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Linear (Fully-connected) Module (1.5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yiJ9vDc2QlrA",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# define parameters using nn.Parameter.\n",
        "# initialize weihgt using nn.init.kaiming_uniform and bias by zeroes\n",
        "# use F.linear in forward method.\n",
        "#################################################################################\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(Linear, self).__init__()\n",
        "        \n",
        "        self.in_size = in_features\n",
        "        self.out_size = out_features\n",
        "\n",
        "        self.W = nn.Parameter(nn.init.kaiming_uniform_(torch.empty(out_features, in_features)))\n",
        "        self.b = nn.Parameter(nn.init.zeros_(torch.empty(out_features)))\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.linear(x, self.W, bias=self.b)\n",
        "        return out\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK6RlWvdWi7o",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 1D Batch Normalization Module (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe0lZDyeWi7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# define and intitialize running_mean and running_var by zeroes and ones\n",
        "# respectively.\n",
        "# define weight and bias using nn.Parameter. initialize weights to a \n",
        "# normal distribution (std=1) and bias to zero.\n",
        "# use F.batch_norm in forward method.\n",
        "# use self.training to differ between training and test phase.\n",
        "#################################################################################\n",
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(BatchNorm, self).__init__()\n",
        "        self.running_mean = nn.init.zeros_(torch.empty(num_features)).to(device)\n",
        "        self.running_var = nn.init.ones_(torch.empty(num_features)).to(device)\n",
        "        self.training = True\n",
        "        self.W = nn.Parameter(nn.init.normal_(torch.empty(num_features)))\n",
        "        self.b = nn.Parameter(nn.init.zeros_(torch.empty(num_features)))\n",
        "\n",
        "        pass\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.batch_norm(x, self.running_mean, self.running_var, weight=self.W, bias=self.b, training=self.training)\n",
        "        return out\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZo4bR0Wi7s",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 2D Batch Normalization Module (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GP3vpuOWi7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# define and intitialize running_mean and running_var by zeroes and ones\n",
        "# respectively.\n",
        "# define weight and bias using nn.Parameter. initialize weights to a\n",
        "# normal distribution (std=1) and bias to zero.\n",
        "# use F.batch_norm in forward method.\n",
        "# use self.training to differ between training and test phase.\n",
        "# more info on 2d batch normalization:\n",
        "# https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network\n",
        "#################################################################################\n",
        "class BatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(BatchNorm2d, self).__init__()\n",
        "        self.running_mean = nn.init.zeros_(torch.empty(num_features)).to(device)\n",
        "        self.running_var = nn.init.ones_(torch.empty(num_features)).to(device)\n",
        "        self.training = True\n",
        "        self.W = nn.Parameter(nn.init.normal_(torch.empty(num_features)))\n",
        "        self.b = nn.Parameter(nn.init.zeros_(torch.empty(num_features)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.batch_norm(x, self.running_mean, self.running_var, weight=self.W, bias=self.b, training=self.training)\n",
        "        return out\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLcxDR75Wi7v",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model (3.5 pts)\n",
        "\n",
        "Using the modules defined in previous part define the following model:\n",
        "\n",
        "`[Conv2d(3, 3), channels=8, stride=1, padding=1] > [BatchNorm2d] > [relu]`\n",
        "\n",
        "`[Conv2d(5, 5), channels=16, stride=1, padding=0] > [BatchNorm2d] > [relu] > [max_pool2d(2, 2), stride=(2, 2), padding=0]`\n",
        "\n",
        "`[Conv2d(5, 5), channels=32, stride=1, padding=0] > [BatchNorm2d] > [relu] > [max_pool2d(2, 2), stride=(2, 2), padding=0]`\n",
        "\n",
        "`[Linear(128)] > [BatchNorm] > [relu]`\n",
        "\n",
        "`[Linear(64)] > [BatchNorm] > [relu]`        __(features)__\n",
        "\n",
        "`[Linear(10)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HCsSJAXKbamz",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dropout=None):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.is_training = True\n",
        "\n",
        "        self.conv1 = Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=1, padding=1)\n",
        "        self.batch1 = BatchNorm2d(num_features=8)\n",
        "\n",
        "        self.conv2 = Conv2d(in_channels=8, out_channels=16, kernel_size=(5,5), stride=1, padding=0)\n",
        "        self.batch2 = BatchNorm2d(num_features=16)\n",
        "\n",
        "        self.conv3 = Conv2d(in_channels=16, out_channels=32, kernel_size=(5,5), stride=1, padding=0)\n",
        "        self.batch3 = BatchNorm2d(num_features=32)\n",
        "\n",
        "        self.linear4 = Linear(in_features=512, out_features=128)\n",
        "        self.batch4 = BatchNorm(128)\n",
        "\n",
        "        self.linear5 = Linear(in_features=128, out_features=64)\n",
        "        self.batch5 = BatchNorm(64)\n",
        "\n",
        "        self.linear6 = Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1.forward(x)\n",
        "        out = self.batch1.forward(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2.forward(out)\n",
        "        out = self.batch2.forward(out)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, kernel_size=(2,2), stride=(2,2), padding=0)\n",
        "\n",
        "        out = self.conv3.forward(out)\n",
        "        out = self.batch3.forward(out)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, kernel_size=(2,2), stride=(2,2), padding=0)\n",
        "\n",
        "        out = out.view(-1, 32 * 4 * 4)\n",
        "\n",
        "        out = self.linear4.forward(out)\n",
        "        out = self.batch4(out)\n",
        "        out = F.relu(out)\n",
        "        if(self.dropout):\n",
        "            out = F.dropout(out, p = 0.5, training=self.is_training)\n",
        "\n",
        "        out = self.linear5.forward(out)\n",
        "        out = self.batch5(out)\n",
        "        features = F.relu(out)\n",
        "        if(self.dropout):\n",
        "            features = F.dropout(features, p = 0.5, training=self.is_training)\n",
        "\n",
        "        out = self.linear6(features)\n",
        "\n",
        "        return out, features\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hRGM3IDWi7z",
        "colab_type": "text"
      },
      "source": [
        "## 4. Training the Model (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qyREDBZgWi7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, trainloader, testloader, device, num_epoches, label):\n",
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# write the main training loop procedure:\n",
        "# move data to defined device\n",
        "# zero_grad optimizer\n",
        "# forward\n",
        "# compute loss using F.cross_entropy\n",
        "# backward\n",
        "# step the optimizer\n",
        "# accumulate running loss\n",
        "#################################################################################\n",
        "    is_training = True\n",
        "    for epoch in range(num_epoches):\n",
        "        print('EPOCH {:2d}:'.format(epoch + 1))\n",
        "        model.train()\n",
        "        running_loss = 0.\n",
        "        for i, (x, y) in enumerate(trainloader):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred, feature = model(x.to(device))\n",
        "            loss = F.cross_entropy(y_pred, y.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x.shape[0]\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "       \n",
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# compute test loss:\n",
        "# dont forget to change model mode from train to eval\n",
        "# write the code in a with torch.no_grad() block to prevent computing and \n",
        "# accumulating gradients\n",
        "# accumulate loss in test_loss variable\n",
        "#################################################################################\n",
        "            if i % 100 == 99:\n",
        "                test_loss = 0.\n",
        "                is_training = model.training\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for (x_t, y_t) in testloader:\n",
        "                        y_pred_t, _ = model(x_t.to(device))\n",
        "                        loss = F.cross_entropy(y_pred_t, y_t.to(device))\n",
        "                        test_loss += loss.item() * x_t.shape[0]\n",
        "\n",
        "                    # test_loss /= len(testloader.sampler)\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "                writer.add_scalars('loss/'+label, \n",
        "                                   {'train': running_loss/100, 'test': test_loss/len(testloader)},\n",
        "                                  global_step=epoch * len(trainloader) + i + 1)\n",
        "                writer.flush()\n",
        "                print('\\titeration {:4d}: training_loss = {:5f}, test_loss = {:5f}'.format(i + 1, running_loss/100, test_loss/len(testloader)))\n",
        "                running_loss = 0.\n",
        "            \n",
        "                model.train(mode = is_training)\n",
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# compute test accuracy:\n",
        "# dont forget to change model mode from train to eval\n",
        "# write the code in a with torch.no_grad() block to prevent computing and \n",
        "# accumulating gradients\n",
        "# accumulate number of correct predictions in correct variable and total test\n",
        "# samples in total variable\n",
        "# accumulate number of classwise correct predictions in class_correct list \n",
        "# and total classwise test samples in class_total list\n",
        "#################################################################################\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            class_correct = [0.] * 10\n",
        "            class_total = [0.] * 10\n",
        "\n",
        "            for (data, target) in testloader:\n",
        "                output, _ = model(data.to(device))\n",
        "\n",
        "                pred = torch.argmax(output, dim = 1)\n",
        "                for idx, k in enumerate(pred):\n",
        "                    class_total[target[idx]] += 1\n",
        "                    if(k == target[idx]):\n",
        "                        class_correct[k] += 1\n",
        "\n",
        "            correct = sum(class_correct)\n",
        "            total = sum(class_total)\n",
        "\n",
        "\n",
        "            pass\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################\n",
        "        writer.add_scalars('accuracy/'+label, {'test': correct / total},\n",
        "                           global_step=(epoch + 1) * len(trainloader))\n",
        "        print('test_accuracy = {:5f}'.format(correct / total))\n",
        "        \n",
        "        writer.add_scalars('classwise_accuracy/'+label, \n",
        "                           {classes[i]: class_correct[i]/class_total[i] for i in range(10)},\n",
        "                           global_step=(epoch + 1) * len(trainloader))\n",
        "        for i in range(10):\n",
        "            print('  >> {:11s}: {:5f}'.format(classes[i], class_correct[i]/class_total[i]))\n",
        "            \n",
        "        writer.flush()\n",
        "        torch.save(model.state_dict(), './saved_models/model_{}.chkpt'.format(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icG5VmKZWi72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd4fc963-16df-47f4-96cc-511c2245eb8f"
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# initilize model and train for 10 epoches using Adam optimizer\n",
        "#################################################################################\n",
        "num_epoches = 10\n",
        "model = Model(dropout = False)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "writer.add_graph(model, images.to(device))\n",
        "train(model, optimizer, trainloader, testloader, device, num_epoches, 'base')\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH  1:\n",
            "\titeration  100: training_loss = 39.446511, test_loss = 26.881039\n",
            "\titeration  200: training_loss = 24.069481, test_loss = 21.521734\n",
            "\titeration  300: training_loss = 21.462413, test_loss = 19.309621\n",
            "\titeration  400: training_loss = 19.161621, test_loss = 18.050663\n",
            "\titeration  500: training_loss = 17.978520, test_loss = 16.673897\n",
            "\titeration  600: training_loss = 17.946598, test_loss = 16.959615\n",
            "\titeration  700: training_loss = 16.863912, test_loss = 16.337858\n",
            "\titeration  800: training_loss = 16.100125, test_loss = 15.573174\n",
            "\titeration  900: training_loss = 15.639371, test_loss = 15.775224\n",
            "\titeration 1000: training_loss = 15.479437, test_loss = 14.610753\n",
            "\titeration 1100: training_loss = 14.423972, test_loss = 14.365429\n",
            "\titeration 1200: training_loss = 15.159401, test_loss = 14.571200\n",
            "\titeration 1300: training_loss = 14.684134, test_loss = 13.278542\n",
            "\titeration 1400: training_loss = 13.932522, test_loss = 14.229921\n",
            "\titeration 1500: training_loss = 14.512334, test_loss = 13.993987\n",
            "\titeration 1600: training_loss = 13.884289, test_loss = 13.107759\n",
            "\titeration 1700: training_loss = 14.102581, test_loss = 13.052761\n",
            "\titeration 1800: training_loss = 13.886988, test_loss = 13.157127\n",
            "test_accuracy = 0.844400\n",
            "  >> T-shirt/top: 0.878000\n",
            "  >> Trouser    : 0.966000\n",
            "  >> Pullover   : 0.708000\n",
            "  >> Dress      : 0.857000\n",
            "  >> Coat       : 0.757000\n",
            "  >> Sandal     : 0.954000\n",
            "  >> Shirt      : 0.482000\n",
            "  >> Sneaker    : 0.962000\n",
            "  >> Bag        : 0.955000\n",
            "  >> Ankle Boot : 0.925000\n",
            "EPOCH  2:\n",
            "\titeration  100: training_loss = 13.006374, test_loss = 12.631763\n",
            "\titeration  200: training_loss = 12.462265, test_loss = 12.613250\n",
            "\titeration  300: training_loss = 13.451258, test_loss = 12.580111\n",
            "\titeration  400: training_loss = 12.576285, test_loss = 12.414764\n",
            "\titeration  500: training_loss = 12.252980, test_loss = 12.270154\n",
            "\titeration  600: training_loss = 12.957536, test_loss = 12.683516\n",
            "\titeration  700: training_loss = 12.732010, test_loss = 12.399813\n",
            "\titeration  800: training_loss = 12.110861, test_loss = 11.692685\n",
            "\titeration  900: training_loss = 12.521294, test_loss = 12.165815\n",
            "\titeration 1000: training_loss = 12.279760, test_loss = 11.911622\n",
            "\titeration 1100: training_loss = 11.829214, test_loss = 12.097779\n",
            "\titeration 1200: training_loss = 11.873787, test_loss = 11.351887\n",
            "\titeration 1300: training_loss = 11.641689, test_loss = 11.590353\n",
            "\titeration 1400: training_loss = 11.556955, test_loss = 11.301303\n",
            "\titeration 1500: training_loss = 12.196908, test_loss = 11.478699\n",
            "\titeration 1600: training_loss = 11.382464, test_loss = 11.442193\n",
            "\titeration 1700: training_loss = 12.156275, test_loss = 11.032015\n",
            "\titeration 1800: training_loss = 11.628375, test_loss = 11.320609\n",
            "test_accuracy = 0.867700\n",
            "  >> T-shirt/top: 0.849000\n",
            "  >> Trouser    : 0.965000\n",
            "  >> Pullover   : 0.733000\n",
            "  >> Dress      : 0.899000\n",
            "  >> Coat       : 0.791000\n",
            "  >> Sandal     : 0.936000\n",
            "  >> Shirt      : 0.613000\n",
            "  >> Sneaker    : 0.971000\n",
            "  >> Bag        : 0.975000\n",
            "  >> Ankle Boot : 0.945000\n",
            "EPOCH  3:\n",
            "\titeration  100: training_loss = 10.876650, test_loss = 11.143154\n",
            "\titeration  200: training_loss = 10.255650, test_loss = 10.900863\n",
            "\titeration  300: training_loss = 11.414991, test_loss = 11.049542\n",
            "\titeration  400: training_loss = 10.449970, test_loss = 10.829656\n",
            "\titeration  500: training_loss = 10.492730, test_loss = 11.070183\n",
            "\titeration  600: training_loss = 11.645072, test_loss = 10.999128\n",
            "\titeration  700: training_loss = 10.608172, test_loss = 11.224328\n",
            "\titeration  800: training_loss = 10.660382, test_loss = 10.793402\n",
            "\titeration  900: training_loss = 10.661908, test_loss = 10.937511\n",
            "\titeration 1000: training_loss = 10.961653, test_loss = 10.838341\n",
            "\titeration 1100: training_loss = 10.209945, test_loss = 10.855056\n",
            "\titeration 1200: training_loss = 10.606279, test_loss = 10.405352\n",
            "\titeration 1300: training_loss = 10.435951, test_loss = 10.548423\n",
            "\titeration 1400: training_loss = 10.509502, test_loss = 10.290852\n",
            "\titeration 1500: training_loss = 10.554186, test_loss = 10.895336\n",
            "\titeration 1600: training_loss = 10.220948, test_loss = 10.637066\n",
            "\titeration 1700: training_loss = 10.488449, test_loss = 10.205291\n",
            "\titeration 1800: training_loss = 10.088150, test_loss = 10.581629\n",
            "test_accuracy = 0.875400\n",
            "  >> T-shirt/top: 0.872000\n",
            "  >> Trouser    : 0.967000\n",
            "  >> Pullover   : 0.766000\n",
            "  >> Dress      : 0.884000\n",
            "  >> Coat       : 0.873000\n",
            "  >> Sandal     : 0.942000\n",
            "  >> Shirt      : 0.551000\n",
            "  >> Sneaker    : 0.980000\n",
            "  >> Bag        : 0.979000\n",
            "  >> Ankle Boot : 0.940000\n",
            "EPOCH  4:\n",
            "\titeration  100: training_loss = 10.144444, test_loss = 10.139355\n",
            "\titeration  200: training_loss = 9.507750, test_loss = 9.832040\n",
            "\titeration  300: training_loss = 10.244039, test_loss = 9.954848\n",
            "\titeration  400: training_loss = 9.985725, test_loss = 10.671850\n",
            "\titeration  500: training_loss = 9.973601, test_loss = 9.957958\n",
            "\titeration  600: training_loss = 10.406133, test_loss = 9.888763\n",
            "\titeration  700: training_loss = 9.731759, test_loss = 10.509300\n",
            "\titeration  800: training_loss = 10.054225, test_loss = 10.371733\n",
            "\titeration  900: training_loss = 10.038480, test_loss = 10.315013\n",
            "\titeration 1000: training_loss = 10.167701, test_loss = 9.837260\n",
            "\titeration 1100: training_loss = 9.625529, test_loss = 10.261643\n",
            "\titeration 1200: training_loss = 9.965404, test_loss = 9.879422\n",
            "\titeration 1300: training_loss = 9.732257, test_loss = 9.852246\n",
            "\titeration 1400: training_loss = 9.635620, test_loss = 9.922954\n",
            "\titeration 1500: training_loss = 9.426153, test_loss = 10.082001\n",
            "\titeration 1600: training_loss = 9.637092, test_loss = 9.920181\n",
            "\titeration 1700: training_loss = 9.429029, test_loss = 9.701836\n",
            "\titeration 1800: training_loss = 9.722157, test_loss = 9.617092\n",
            "test_accuracy = 0.870600\n",
            "  >> T-shirt/top: 0.801000\n",
            "  >> Trouser    : 0.973000\n",
            "  >> Pullover   : 0.660000\n",
            "  >> Dress      : 0.897000\n",
            "  >> Coat       : 0.848000\n",
            "  >> Sandal     : 0.966000\n",
            "  >> Shirt      : 0.671000\n",
            "  >> Sneaker    : 0.970000\n",
            "  >> Bag        : 0.979000\n",
            "  >> Ankle Boot : 0.941000\n",
            "EPOCH  5:\n",
            "\titeration  100: training_loss = 9.254690, test_loss = 9.603122\n",
            "\titeration  200: training_loss = 9.166118, test_loss = 9.890656\n",
            "\titeration  300: training_loss = 9.765465, test_loss = 10.003562\n",
            "\titeration  400: training_loss = 9.277182, test_loss = 10.114213\n",
            "\titeration  500: training_loss = 9.165822, test_loss = 9.482256\n",
            "\titeration  600: training_loss = 10.008957, test_loss = 9.698718\n",
            "\titeration  700: training_loss = 9.471754, test_loss = 10.488968\n",
            "\titeration  800: training_loss = 9.381684, test_loss = 10.067776\n",
            "\titeration  900: training_loss = 9.284518, test_loss = 9.668860\n",
            "\titeration 1000: training_loss = 9.656018, test_loss = 9.617657\n",
            "\titeration 1100: training_loss = 8.995487, test_loss = 9.742276\n",
            "\titeration 1200: training_loss = 9.137342, test_loss = 9.392218\n",
            "\titeration 1300: training_loss = 9.036612, test_loss = 9.497994\n",
            "\titeration 1400: training_loss = 9.092502, test_loss = 9.378515\n",
            "\titeration 1500: training_loss = 9.339126, test_loss = 9.357678\n",
            "\titeration 1600: training_loss = 8.968956, test_loss = 9.579859\n",
            "\titeration 1700: training_loss = 9.162462, test_loss = 9.252340\n",
            "\titeration 1800: training_loss = 9.145313, test_loss = 9.343504\n",
            "test_accuracy = 0.888700\n",
            "  >> T-shirt/top: 0.867000\n",
            "  >> Trouser    : 0.972000\n",
            "  >> Pullover   : 0.797000\n",
            "  >> Dress      : 0.905000\n",
            "  >> Coat       : 0.846000\n",
            "  >> Sandal     : 0.951000\n",
            "  >> Shirt      : 0.651000\n",
            "  >> Sneaker    : 0.964000\n",
            "  >> Bag        : 0.980000\n",
            "  >> Ankle Boot : 0.954000\n",
            "EPOCH  6:\n",
            "\titeration  100: training_loss = 8.603991, test_loss = 9.831868\n",
            "\titeration  200: training_loss = 8.350361, test_loss = 9.536176\n",
            "\titeration  300: training_loss = 9.092462, test_loss = 9.203037\n",
            "\titeration  400: training_loss = 8.805629, test_loss = 9.912290\n",
            "\titeration  500: training_loss = 8.363232, test_loss = 9.217462\n",
            "\titeration  600: training_loss = 9.273714, test_loss = 9.481868\n",
            "\titeration  700: training_loss = 8.766911, test_loss = 9.777022\n",
            "\titeration  800: training_loss = 9.207322, test_loss = 9.239512\n",
            "\titeration  900: training_loss = 8.501844, test_loss = 9.359663\n",
            "\titeration 1000: training_loss = 9.035796, test_loss = 9.125447\n",
            "\titeration 1100: training_loss = 8.530543, test_loss = 9.274048\n",
            "\titeration 1200: training_loss = 8.918422, test_loss = 9.312274\n",
            "\titeration 1300: training_loss = 8.227002, test_loss = 9.162351\n",
            "\titeration 1400: training_loss = 8.527748, test_loss = 9.209327\n",
            "\titeration 1500: training_loss = 8.665056, test_loss = 9.027876\n",
            "\titeration 1600: training_loss = 8.384798, test_loss = 9.160575\n",
            "\titeration 1700: training_loss = 8.875082, test_loss = 9.193018\n",
            "\titeration 1800: training_loss = 8.871470, test_loss = 9.407833\n",
            "test_accuracy = 0.885600\n",
            "  >> T-shirt/top: 0.799000\n",
            "  >> Trouser    : 0.979000\n",
            "  >> Pullover   : 0.797000\n",
            "  >> Dress      : 0.910000\n",
            "  >> Coat       : 0.831000\n",
            "  >> Sandal     : 0.955000\n",
            "  >> Shirt      : 0.682000\n",
            "  >> Sneaker    : 0.981000\n",
            "  >> Bag        : 0.982000\n",
            "  >> Ankle Boot : 0.940000\n",
            "EPOCH  7:\n",
            "\titeration  100: training_loss = 8.540138, test_loss = 9.577827\n",
            "\titeration  200: training_loss = 7.949107, test_loss = 9.142387\n",
            "\titeration  300: training_loss = 8.742574, test_loss = 9.015203\n",
            "\titeration  400: training_loss = 8.103898, test_loss = 9.327907\n",
            "\titeration  500: training_loss = 8.141875, test_loss = 9.005782\n",
            "\titeration  600: training_loss = 9.280640, test_loss = 9.144543\n",
            "\titeration  700: training_loss = 8.194707, test_loss = 9.116486\n",
            "\titeration  800: training_loss = 8.377898, test_loss = 9.094562\n",
            "\titeration  900: training_loss = 7.996677, test_loss = 8.870713\n",
            "\titeration 1000: training_loss = 8.867783, test_loss = 9.491110\n",
            "\titeration 1100: training_loss = 7.808008, test_loss = 9.340702\n",
            "\titeration 1200: training_loss = 8.286390, test_loss = 9.173968\n",
            "\titeration 1300: training_loss = 8.394198, test_loss = 9.144795\n",
            "\titeration 1400: training_loss = 8.466142, test_loss = 9.123828\n",
            "\titeration 1500: training_loss = 8.081250, test_loss = 9.279698\n",
            "\titeration 1600: training_loss = 7.966528, test_loss = 9.184521\n",
            "\titeration 1700: training_loss = 8.504119, test_loss = 8.984276\n",
            "\titeration 1800: training_loss = 8.144661, test_loss = 8.759704\n",
            "test_accuracy = 0.887200\n",
            "  >> T-shirt/top: 0.806000\n",
            "  >> Trouser    : 0.968000\n",
            "  >> Pullover   : 0.735000\n",
            "  >> Dress      : 0.934000\n",
            "  >> Coat       : 0.872000\n",
            "  >> Sandal     : 0.970000\n",
            "  >> Shirt      : 0.688000\n",
            "  >> Sneaker    : 0.980000\n",
            "  >> Bag        : 0.984000\n",
            "  >> Ankle Boot : 0.935000\n",
            "EPOCH  8:\n",
            "\titeration  100: training_loss = 8.125426, test_loss = 8.823894\n",
            "\titeration  200: training_loss = 7.756577, test_loss = 9.296867\n",
            "\titeration  300: training_loss = 8.550073, test_loss = 9.152669\n",
            "\titeration  400: training_loss = 7.738494, test_loss = 8.993058\n",
            "\titeration  500: training_loss = 8.084800, test_loss = 8.941380\n",
            "\titeration  600: training_loss = 8.739286, test_loss = 9.457543\n",
            "\titeration  700: training_loss = 7.865102, test_loss = 9.059092\n",
            "\titeration  800: training_loss = 8.232180, test_loss = 8.928146\n",
            "\titeration  900: training_loss = 8.046073, test_loss = 8.877013\n",
            "\titeration 1000: training_loss = 8.627974, test_loss = 8.747173\n",
            "\titeration 1100: training_loss = 7.841683, test_loss = 8.785041\n",
            "\titeration 1200: training_loss = 7.889617, test_loss = 8.519985\n",
            "\titeration 1300: training_loss = 7.949689, test_loss = 9.486275\n",
            "\titeration 1400: training_loss = 7.788951, test_loss = 8.790291\n",
            "\titeration 1500: training_loss = 8.263815, test_loss = 9.303080\n",
            "\titeration 1600: training_loss = 7.825723, test_loss = 8.830962\n",
            "\titeration 1700: training_loss = 8.284441, test_loss = 8.585830\n",
            "\titeration 1800: training_loss = 8.361326, test_loss = 8.910501\n",
            "test_accuracy = 0.893800\n",
            "  >> T-shirt/top: 0.874000\n",
            "  >> Trouser    : 0.982000\n",
            "  >> Pullover   : 0.809000\n",
            "  >> Dress      : 0.893000\n",
            "  >> Coat       : 0.824000\n",
            "  >> Sandal     : 0.972000\n",
            "  >> Shirt      : 0.677000\n",
            "  >> Sneaker    : 0.967000\n",
            "  >> Bag        : 0.981000\n",
            "  >> Ankle Boot : 0.959000\n",
            "EPOCH  9:\n",
            "\titeration  100: training_loss = 7.488702, test_loss = 8.583914\n",
            "\titeration  200: training_loss = 7.405015, test_loss = 8.528781\n",
            "\titeration  300: training_loss = 8.118488, test_loss = 8.987610\n",
            "\titeration  400: training_loss = 7.875826, test_loss = 9.019963\n",
            "\titeration  500: training_loss = 7.567287, test_loss = 8.629430\n",
            "\titeration  600: training_loss = 8.466189, test_loss = 8.646711\n",
            "\titeration  700: training_loss = 7.695322, test_loss = 8.767335\n",
            "\titeration  800: training_loss = 7.869714, test_loss = 8.778317\n",
            "\titeration  900: training_loss = 7.408570, test_loss = 8.488418\n",
            "\titeration 1000: training_loss = 8.300033, test_loss = 8.571722\n",
            "\titeration 1100: training_loss = 7.498907, test_loss = 8.802942\n",
            "\titeration 1200: training_loss = 8.164833, test_loss = 8.443934\n",
            "\titeration 1300: training_loss = 7.746003, test_loss = 8.999720\n",
            "\titeration 1400: training_loss = 7.735637, test_loss = 8.609424\n",
            "\titeration 1500: training_loss = 7.923953, test_loss = 8.709454\n",
            "\titeration 1600: training_loss = 7.603232, test_loss = 8.769485\n",
            "\titeration 1700: training_loss = 7.635414, test_loss = 8.528535\n",
            "\titeration 1800: training_loss = 7.961670, test_loss = 8.703715\n",
            "test_accuracy = 0.900900\n",
            "  >> T-shirt/top: 0.867000\n",
            "  >> Trouser    : 0.978000\n",
            "  >> Pullover   : 0.831000\n",
            "  >> Dress      : 0.917000\n",
            "  >> Coat       : 0.848000\n",
            "  >> Sandal     : 0.964000\n",
            "  >> Shirt      : 0.700000\n",
            "  >> Sneaker    : 0.966000\n",
            "  >> Bag        : 0.981000\n",
            "  >> Ankle Boot : 0.957000\n",
            "EPOCH 10:\n",
            "\titeration  100: training_loss = 7.116218, test_loss = 8.665636\n",
            "\titeration  200: training_loss = 7.230964, test_loss = 8.736115\n",
            "\titeration  300: training_loss = 7.749361, test_loss = 8.574366\n",
            "\titeration  400: training_loss = 7.844858, test_loss = 8.723652\n",
            "\titeration  500: training_loss = 7.704289, test_loss = 8.541043\n",
            "\titeration  600: training_loss = 8.136454, test_loss = 8.992238\n",
            "\titeration  700: training_loss = 7.504558, test_loss = 8.719261\n",
            "\titeration  800: training_loss = 7.579072, test_loss = 8.501494\n",
            "\titeration  900: training_loss = 7.181470, test_loss = 8.618473\n",
            "\titeration 1000: training_loss = 7.645951, test_loss = 8.504341\n",
            "\titeration 1100: training_loss = 7.408433, test_loss = 8.480158\n",
            "\titeration 1200: training_loss = 7.444172, test_loss = 8.640990\n",
            "\titeration 1300: training_loss = 7.304831, test_loss = 8.577635\n",
            "\titeration 1400: training_loss = 7.003264, test_loss = 8.520175\n",
            "\titeration 1500: training_loss = 7.101098, test_loss = 8.486418\n",
            "\titeration 1600: training_loss = 7.225204, test_loss = 8.447700\n",
            "\titeration 1700: training_loss = 7.668118, test_loss = 8.478570\n",
            "\titeration 1800: training_loss = 7.675747, test_loss = 8.598187\n",
            "test_accuracy = 0.903900\n",
            "  >> T-shirt/top: 0.879000\n",
            "  >> Trouser    : 0.976000\n",
            "  >> Pullover   : 0.814000\n",
            "  >> Dress      : 0.902000\n",
            "  >> Coat       : 0.886000\n",
            "  >> Sandal     : 0.969000\n",
            "  >> Shirt      : 0.695000\n",
            "  >> Sneaker    : 0.969000\n",
            "  >> Bag        : 0.985000\n",
            "  >> Ankle Boot : 0.964000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJVizp04Wi74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# add model features corresponding to nimages as embedding to tensorboard\n",
        "#################################################################################\n",
        "pass\n",
        "writer.flush()\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5B2ypciWi76",
        "colab_type": "text"
      },
      "source": [
        "## 5. Dropout and Data Augmentation (6 pts)\n",
        "\n",
        "Add dropout with p=0.5 to first two linear layers of the model using `F.dropout`. You can either modify the model module to take an additional parameter or write a seperate module. \n",
        "\n",
        "Data Augmentation is a strategy for increasing dataset size to prevent overfitting and better generalization. Dataset can be augmented by any transformation on data that do not change its label.\n",
        "\n",
        "Pytorch provides data augmentation transforms in `torchvision.transforms` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPLFEkNaWi77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# compose a transform using transforms.Compose that horizontally flips images \n",
        "# and use transforms.RandomResizedCrop to crop a 20 * 20 patch of the image \n",
        "# and resizing back to 28 * 28\n",
        "#################################################################################\n",
        "transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 1), \n",
        "                                transforms.RandomResizedCrop(28, scale=(20/28, 1), ratio=(1,1)), \n",
        "                               transforms.ToTensor()]\n",
        "                               )\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = datasets.FashionMNIST(root='./data',\n",
        "                                    train=True,\n",
        "                                    download=True,\n",
        "                                    transform=transform)\n",
        "\n",
        "testset = datasets.FashionMNIST(root='./data',\n",
        "                                   train=False,\n",
        "                                   download=True,\n",
        "                                   transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n",
        "\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdAsxVDNWi79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "img_grid = utils.make_grid(images[:16], nrow=4)\n",
        "\n",
        "writer.add_image('FashionMNIST/augmented', img_grid)\n",
        "writer.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAz2wAJyWi7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6e15f3c-171f-41aa-97c6-6759d779d38d"
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# initilize model and train using augmented dataset for 10 epoches\n",
        "#################################################################################\n",
        "num_epoches = 10\n",
        "model2 = Model(dropout = True)\n",
        "model2.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model2.parameters())\n",
        "\n",
        "writer.add_graph(model2, images.to(device))\n",
        "train(model2, optimizer, trainloader, testloader, device, num_epoches, 'dropout')\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1044: TracerWarning: Trace had nondeterministic nodes. Did you forget call .eval() on your model? Nodes:\n",
            "\t%input.13 : Float(32, 128) = aten::dropout(%input.12, %169, %170) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:807:0\n",
            "\t%input : Float(32, 64) = aten::dropout(%input.16, %184, %185) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:807:0\n",
            "This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[27, 7] (18.296180725097656 vs. 1.8828994035720825) and 319 other locations (100.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 2. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Not within tolerance rtol=1e-05 atol=1e-05 at input[27, 39] (36.843589782714844 vs. 0.0) and 777 other locations (37.00%)\n",
            "  check_tolerance, _force_outplace, True, _module_class)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH  1:\n",
            "\titeration  100: training_loss = 67.220806, test_loss = 53.943564\n",
            "\titeration  200: training_loss = 48.266687, test_loss = 41.780737\n",
            "\titeration  300: training_loss = 39.520360, test_loss = 35.435418\n",
            "\titeration  400: training_loss = 34.763588, test_loss = 32.095810\n",
            "\titeration  500: training_loss = 31.692995, test_loss = 29.405137\n",
            "\titeration  600: training_loss = 30.571603, test_loss = 27.748336\n",
            "\titeration  700: training_loss = 28.042048, test_loss = 26.188073\n",
            "\titeration  800: training_loss = 26.657152, test_loss = 24.731947\n",
            "\titeration  900: training_loss = 25.435404, test_loss = 23.887915\n",
            "\titeration 1000: training_loss = 24.130634, test_loss = 22.661997\n",
            "\titeration 1100: training_loss = 23.028777, test_loss = 21.974150\n",
            "\titeration 1200: training_loss = 22.931733, test_loss = 21.311382\n",
            "\titeration 1300: training_loss = 22.214107, test_loss = 20.536878\n",
            "\titeration 1400: training_loss = 21.582360, test_loss = 20.447222\n",
            "\titeration 1500: training_loss = 22.479642, test_loss = 20.145850\n",
            "\titeration 1600: training_loss = 20.700502, test_loss = 19.839833\n",
            "\titeration 1700: training_loss = 21.430324, test_loss = 19.562025\n",
            "\titeration 1800: training_loss = 20.823653, test_loss = 18.793189\n",
            "test_accuracy = 0.788900\n",
            "  >> T-shirt/top: 0.807000\n",
            "  >> Trouser    : 0.943000\n",
            "  >> Pullover   : 0.671000\n",
            "  >> Dress      : 0.829000\n",
            "  >> Coat       : 0.703000\n",
            "  >> Sandal     : 0.899000\n",
            "  >> Shirt      : 0.246000\n",
            "  >> Sneaker    : 0.919000\n",
            "  >> Bag        : 0.950000\n",
            "  >> Ankle Boot : 0.922000\n",
            "EPOCH  2:\n",
            "\titeration  100: training_loss = 19.577278, test_loss = 19.143610\n",
            "\titeration  200: training_loss = 19.091474, test_loss = 19.104354\n",
            "\titeration  300: training_loss = 19.477219, test_loss = 18.139383\n",
            "\titeration  400: training_loss = 18.976907, test_loss = 18.226189\n",
            "\titeration  500: training_loss = 18.222900, test_loss = 18.277153\n",
            "\titeration  600: training_loss = 19.655807, test_loss = 17.663143\n",
            "\titeration  700: training_loss = 18.069576, test_loss = 17.728068\n",
            "\titeration  800: training_loss = 18.209919, test_loss = 17.572020\n",
            "\titeration  900: training_loss = 18.212948, test_loss = 17.253220\n",
            "\titeration 1000: training_loss = 17.564407, test_loss = 17.083646\n",
            "\titeration 1100: training_loss = 16.543221, test_loss = 16.903587\n",
            "\titeration 1200: training_loss = 17.411106, test_loss = 17.087581\n",
            "\titeration 1300: training_loss = 16.939858, test_loss = 16.451249\n",
            "\titeration 1400: training_loss = 17.116453, test_loss = 16.651186\n",
            "\titeration 1500: training_loss = 18.136443, test_loss = 16.510410\n",
            "\titeration 1600: training_loss = 16.314939, test_loss = 16.589640\n",
            "\titeration 1700: training_loss = 17.966331, test_loss = 16.013263\n",
            "\titeration 1800: training_loss = 16.168819, test_loss = 16.542319\n",
            "test_accuracy = 0.828600\n",
            "  >> T-shirt/top: 0.837000\n",
            "  >> Trouser    : 0.961000\n",
            "  >> Pullover   : 0.732000\n",
            "  >> Dress      : 0.845000\n",
            "  >> Coat       : 0.746000\n",
            "  >> Sandal     : 0.910000\n",
            "  >> Shirt      : 0.421000\n",
            "  >> Sneaker    : 0.956000\n",
            "  >> Bag        : 0.948000\n",
            "  >> Ankle Boot : 0.930000\n",
            "EPOCH  3:\n",
            "\titeration  100: training_loss = 16.269583, test_loss = 15.662927\n",
            "\titeration  200: training_loss = 15.254780, test_loss = 15.930122\n",
            "\titeration  300: training_loss = 17.067029, test_loss = 15.930780\n",
            "\titeration  400: training_loss = 16.357826, test_loss = 15.604871\n",
            "\titeration  500: training_loss = 15.957853, test_loss = 15.854734\n",
            "\titeration  600: training_loss = 17.087671, test_loss = 15.441729\n",
            "\titeration  700: training_loss = 15.962935, test_loss = 15.484209\n",
            "\titeration  800: training_loss = 15.753335, test_loss = 15.165131\n",
            "\titeration  900: training_loss = 15.937471, test_loss = 15.541729\n",
            "\titeration 1000: training_loss = 15.353759, test_loss = 15.193477\n",
            "\titeration 1100: training_loss = 15.458163, test_loss = 15.420495\n",
            "\titeration 1200: training_loss = 16.032706, test_loss = 15.354137\n",
            "\titeration 1300: training_loss = 15.097003, test_loss = 14.659481\n",
            "\titeration 1400: training_loss = 15.760434, test_loss = 15.123745\n",
            "\titeration 1500: training_loss = 16.411773, test_loss = 15.156260\n",
            "\titeration 1600: training_loss = 15.385897, test_loss = 14.686431\n",
            "\titeration 1700: training_loss = 15.287726, test_loss = 15.085946\n",
            "\titeration 1800: training_loss = 14.686552, test_loss = 15.143679\n",
            "test_accuracy = 0.827000\n",
            "  >> T-shirt/top: 0.850000\n",
            "  >> Trouser    : 0.960000\n",
            "  >> Pullover   : 0.721000\n",
            "  >> Dress      : 0.815000\n",
            "  >> Coat       : 0.734000\n",
            "  >> Sandal     : 0.831000\n",
            "  >> Shirt      : 0.521000\n",
            "  >> Sneaker    : 0.989000\n",
            "  >> Bag        : 0.955000\n",
            "  >> Ankle Boot : 0.894000\n",
            "EPOCH  4:\n",
            "\titeration  100: training_loss = 14.280139, test_loss = 14.349725\n",
            "\titeration  200: training_loss = 14.825112, test_loss = 13.994610\n",
            "\titeration  300: training_loss = 15.148042, test_loss = 14.516875\n",
            "\titeration  400: training_loss = 14.511458, test_loss = 14.236925\n",
            "\titeration  500: training_loss = 14.287178, test_loss = 14.583239\n",
            "\titeration  600: training_loss = 15.222889, test_loss = 13.825999\n",
            "\titeration  700: training_loss = 14.572290, test_loss = 14.485154\n",
            "\titeration  800: training_loss = 14.789032, test_loss = 14.027288\n",
            "\titeration  900: training_loss = 14.475645, test_loss = 14.379899\n",
            "\titeration 1000: training_loss = 14.323953, test_loss = 14.453898\n",
            "\titeration 1100: training_loss = 13.719564, test_loss = 13.925620\n",
            "\titeration 1200: training_loss = 14.433592, test_loss = 13.531711\n",
            "\titeration 1300: training_loss = 14.350883, test_loss = 13.909590\n",
            "\titeration 1400: training_loss = 14.198223, test_loss = 14.052442\n",
            "\titeration 1500: training_loss = 14.743714, test_loss = 13.463749\n",
            "\titeration 1600: training_loss = 13.682004, test_loss = 13.628370\n",
            "\titeration 1700: training_loss = 14.765472, test_loss = 13.606692\n",
            "\titeration 1800: training_loss = 14.251584, test_loss = 13.415379\n",
            "test_accuracy = 0.853000\n",
            "  >> T-shirt/top: 0.841000\n",
            "  >> Trouser    : 0.956000\n",
            "  >> Pullover   : 0.781000\n",
            "  >> Dress      : 0.865000\n",
            "  >> Coat       : 0.776000\n",
            "  >> Sandal     : 0.937000\n",
            "  >> Shirt      : 0.537000\n",
            "  >> Sneaker    : 0.971000\n",
            "  >> Bag        : 0.959000\n",
            "  >> Ankle Boot : 0.907000\n",
            "EPOCH  5:\n",
            "\titeration  100: training_loss = 14.084127, test_loss = 13.416107\n",
            "\titeration  200: training_loss = 13.236412, test_loss = 13.529599\n",
            "\titeration  300: training_loss = 13.796535, test_loss = 13.157222\n",
            "\titeration  400: training_loss = 13.934434, test_loss = 13.872628\n",
            "\titeration  500: training_loss = 14.036185, test_loss = 13.291580\n",
            "\titeration  600: training_loss = 14.900904, test_loss = 13.559687\n",
            "\titeration  700: training_loss = 13.471887, test_loss = 13.368927\n",
            "\titeration  800: training_loss = 13.804866, test_loss = 13.652709\n",
            "\titeration  900: training_loss = 13.364682, test_loss = 13.663606\n",
            "\titeration 1000: training_loss = 13.299814, test_loss = 12.964442\n",
            "\titeration 1100: training_loss = 12.946778, test_loss = 13.576472\n",
            "\titeration 1200: training_loss = 13.430422, test_loss = 13.251568\n",
            "\titeration 1300: training_loss = 13.870628, test_loss = 13.007935\n",
            "\titeration 1400: training_loss = 13.003900, test_loss = 13.366725\n",
            "\titeration 1500: training_loss = 14.274494, test_loss = 13.193505\n",
            "\titeration 1600: training_loss = 12.988805, test_loss = 13.331168\n",
            "\titeration 1700: training_loss = 13.410680, test_loss = 12.851383\n",
            "\titeration 1800: training_loss = 13.478949, test_loss = 13.094480\n",
            "test_accuracy = 0.862300\n",
            "  >> T-shirt/top: 0.853000\n",
            "  >> Trouser    : 0.958000\n",
            "  >> Pullover   : 0.804000\n",
            "  >> Dress      : 0.878000\n",
            "  >> Coat       : 0.814000\n",
            "  >> Sandal     : 0.950000\n",
            "  >> Shirt      : 0.507000\n",
            "  >> Sneaker    : 0.942000\n",
            "  >> Bag        : 0.970000\n",
            "  >> Ankle Boot : 0.947000\n",
            "EPOCH  6:\n",
            "\titeration  100: training_loss = 13.060352, test_loss = 13.010132\n",
            "\titeration  200: training_loss = 12.869149, test_loss = 13.253399\n",
            "\titeration  300: training_loss = 12.740541, test_loss = 12.804289\n",
            "\titeration  400: training_loss = 12.601623, test_loss = 12.838415\n",
            "\titeration  500: training_loss = 12.872580, test_loss = 12.741986\n",
            "\titeration  600: training_loss = 13.455712, test_loss = 12.841437\n",
            "\titeration  700: training_loss = 12.976858, test_loss = 13.004439\n",
            "\titeration  800: training_loss = 12.958875, test_loss = 13.062647\n",
            "\titeration  900: training_loss = 13.128066, test_loss = 12.561184\n",
            "\titeration 1000: training_loss = 12.741323, test_loss = 12.974670\n",
            "\titeration 1100: training_loss = 12.295205, test_loss = 12.478763\n",
            "\titeration 1200: training_loss = 13.196912, test_loss = 13.391555\n",
            "\titeration 1300: training_loss = 13.491273, test_loss = 12.609197\n",
            "\titeration 1400: training_loss = 12.711560, test_loss = 12.477964\n",
            "\titeration 1500: training_loss = 14.493244, test_loss = 12.896039\n",
            "\titeration 1600: training_loss = 12.320164, test_loss = 12.959473\n",
            "\titeration 1700: training_loss = 13.282663, test_loss = 12.749372\n",
            "\titeration 1800: training_loss = 13.219505, test_loss = 12.793876\n",
            "test_accuracy = 0.858400\n",
            "  >> T-shirt/top: 0.841000\n",
            "  >> Trouser    : 0.959000\n",
            "  >> Pullover   : 0.771000\n",
            "  >> Dress      : 0.861000\n",
            "  >> Coat       : 0.816000\n",
            "  >> Sandal     : 0.884000\n",
            "  >> Shirt      : 0.576000\n",
            "  >> Sneaker    : 0.955000\n",
            "  >> Bag        : 0.964000\n",
            "  >> Ankle Boot : 0.957000\n",
            "EPOCH  7:\n",
            "\titeration  100: training_loss = 12.488883, test_loss = 12.775298\n",
            "\titeration  200: training_loss = 12.033247, test_loss = 12.788982\n",
            "\titeration  300: training_loss = 12.847365, test_loss = 12.324061\n",
            "\titeration  400: training_loss = 12.923260, test_loss = 12.956881\n",
            "\titeration  500: training_loss = 12.684138, test_loss = 12.481060\n",
            "\titeration  600: training_loss = 13.734369, test_loss = 12.408567\n",
            "\titeration  700: training_loss = 11.947730, test_loss = 12.165481\n",
            "\titeration  800: training_loss = 12.591713, test_loss = 12.630956\n",
            "\titeration  900: training_loss = 12.880374, test_loss = 12.157543\n",
            "\titeration 1000: training_loss = 12.366877, test_loss = 12.042074\n",
            "\titeration 1100: training_loss = 12.022122, test_loss = 12.154260\n",
            "\titeration 1200: training_loss = 12.110187, test_loss = 12.261411\n",
            "\titeration 1300: training_loss = 12.710815, test_loss = 12.237564\n",
            "\titeration 1400: training_loss = 12.250283, test_loss = 12.380782\n",
            "\titeration 1500: training_loss = 12.933717, test_loss = 12.603770\n",
            "\titeration 1600: training_loss = 12.220144, test_loss = 12.445269\n",
            "\titeration 1700: training_loss = 12.677560, test_loss = 12.342114\n",
            "\titeration 1800: training_loss = 12.265432, test_loss = 12.221473\n",
            "test_accuracy = 0.864700\n",
            "  >> T-shirt/top: 0.850000\n",
            "  >> Trouser    : 0.961000\n",
            "  >> Pullover   : 0.778000\n",
            "  >> Dress      : 0.863000\n",
            "  >> Coat       : 0.817000\n",
            "  >> Sandal     : 0.927000\n",
            "  >> Shirt      : 0.581000\n",
            "  >> Sneaker    : 0.968000\n",
            "  >> Bag        : 0.961000\n",
            "  >> Ankle Boot : 0.941000\n",
            "EPOCH  8:\n",
            "\titeration  100: training_loss = 12.383676, test_loss = 12.418849\n",
            "\titeration  200: training_loss = 11.688897, test_loss = 12.397821\n",
            "\titeration  300: training_loss = 12.274050, test_loss = 12.132982\n",
            "\titeration  400: training_loss = 12.250030, test_loss = 12.190616\n",
            "\titeration  500: training_loss = 11.647854, test_loss = 12.008348\n",
            "\titeration  600: training_loss = 13.058814, test_loss = 12.360177\n",
            "\titeration  700: training_loss = 11.864146, test_loss = 11.834847\n",
            "\titeration  800: training_loss = 11.564089, test_loss = 12.000161\n",
            "\titeration  900: training_loss = 11.845085, test_loss = 12.497689\n",
            "\titeration 1000: training_loss = 12.601912, test_loss = 11.905748\n",
            "\titeration 1100: training_loss = 11.552324, test_loss = 11.937602\n",
            "\titeration 1200: training_loss = 12.172286, test_loss = 11.791650\n",
            "\titeration 1300: training_loss = 12.000082, test_loss = 12.220495\n",
            "\titeration 1400: training_loss = 11.941515, test_loss = 12.158833\n",
            "\titeration 1500: training_loss = 11.852174, test_loss = 12.174272\n",
            "\titeration 1600: training_loss = 12.388813, test_loss = 12.181923\n",
            "\titeration 1700: training_loss = 12.676646, test_loss = 11.863018\n",
            "\titeration 1800: training_loss = 12.558334, test_loss = 11.808852\n",
            "test_accuracy = 0.873500\n",
            "  >> T-shirt/top: 0.868000\n",
            "  >> Trouser    : 0.966000\n",
            "  >> Pullover   : 0.798000\n",
            "  >> Dress      : 0.889000\n",
            "  >> Coat       : 0.796000\n",
            "  >> Sandal     : 0.943000\n",
            "  >> Shirt      : 0.609000\n",
            "  >> Sneaker    : 0.938000\n",
            "  >> Bag        : 0.971000\n",
            "  >> Ankle Boot : 0.957000\n",
            "EPOCH  9:\n",
            "\titeration  100: training_loss = 11.902969, test_loss = 12.116202\n",
            "\titeration  200: training_loss = 11.677757, test_loss = 12.202927\n",
            "\titeration  300: training_loss = 11.676958, test_loss = 11.706841\n",
            "\titeration  400: training_loss = 11.970978, test_loss = 11.682775\n",
            "\titeration  500: training_loss = 11.277413, test_loss = 11.977558\n",
            "\titeration  600: training_loss = 12.867864, test_loss = 12.269836\n",
            "\titeration  700: training_loss = 11.884475, test_loss = 12.157640\n",
            "\titeration  800: training_loss = 12.237375, test_loss = 12.002890\n",
            "\titeration  900: training_loss = 12.498933, test_loss = 11.938803\n",
            "\titeration 1000: training_loss = 11.926337, test_loss = 11.843228\n",
            "\titeration 1100: training_loss = 11.518902, test_loss = 11.491990\n",
            "\titeration 1200: training_loss = 11.591951, test_loss = 11.577787\n",
            "\titeration 1300: training_loss = 11.786856, test_loss = 11.785599\n",
            "\titeration 1400: training_loss = 11.695090, test_loss = 11.542871\n",
            "\titeration 1500: training_loss = 12.319489, test_loss = 11.796952\n",
            "\titeration 1600: training_loss = 11.612234, test_loss = 11.658379\n",
            "\titeration 1700: training_loss = 11.744823, test_loss = 11.544650\n",
            "\titeration 1800: training_loss = 11.891080, test_loss = 11.521945\n",
            "test_accuracy = 0.872000\n",
            "  >> T-shirt/top: 0.825000\n",
            "  >> Trouser    : 0.958000\n",
            "  >> Pullover   : 0.775000\n",
            "  >> Dress      : 0.880000\n",
            "  >> Coat       : 0.808000\n",
            "  >> Sandal     : 0.957000\n",
            "  >> Shirt      : 0.643000\n",
            "  >> Sneaker    : 0.943000\n",
            "  >> Bag        : 0.976000\n",
            "  >> Ankle Boot : 0.955000\n",
            "EPOCH 10:\n",
            "\titeration  100: training_loss = 11.621756, test_loss = 11.997200\n",
            "\titeration  200: training_loss = 11.324524, test_loss = 11.986136\n",
            "\titeration  300: training_loss = 11.072264, test_loss = 11.953132\n",
            "\titeration  400: training_loss = 11.369738, test_loss = 11.477319\n",
            "\titeration  500: training_loss = 11.434755, test_loss = 11.847981\n",
            "\titeration  600: training_loss = 12.755314, test_loss = 11.634578\n",
            "\titeration  700: training_loss = 11.460766, test_loss = 11.725284\n",
            "\titeration  800: training_loss = 11.693923, test_loss = 11.549411\n",
            "\titeration  900: training_loss = 11.329449, test_loss = 11.890545\n",
            "\titeration 1000: training_loss = 12.049718, test_loss = 11.525929\n",
            "\titeration 1100: training_loss = 11.190121, test_loss = 11.665887\n",
            "\titeration 1200: training_loss = 11.338052, test_loss = 12.016735\n",
            "\titeration 1300: training_loss = 11.292071, test_loss = 11.396145\n",
            "\titeration 1400: training_loss = 11.090000, test_loss = 11.874694\n",
            "\titeration 1500: training_loss = 11.451938, test_loss = 11.901080\n",
            "\titeration 1600: training_loss = 11.941930, test_loss = 11.595187\n",
            "\titeration 1700: training_loss = 11.772070, test_loss = 11.498334\n",
            "\titeration 1800: training_loss = 11.586758, test_loss = 11.749082\n",
            "test_accuracy = 0.877800\n",
            "  >> T-shirt/top: 0.812000\n",
            "  >> Trouser    : 0.971000\n",
            "  >> Pullover   : 0.808000\n",
            "  >> Dress      : 0.884000\n",
            "  >> Coat       : 0.823000\n",
            "  >> Sandal     : 0.965000\n",
            "  >> Shirt      : 0.620000\n",
            "  >> Sneaker    : 0.959000\n",
            "  >> Bag        : 0.975000\n",
            "  >> Ankle Boot : 0.961000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FWwGvwiWi8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# add model features corresponding to nimages as embedding to tensorboard\n",
        "#################################################################################\n",
        "pass\n",
        "\n",
        "writer.flush()\n",
        "writer.close()\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}