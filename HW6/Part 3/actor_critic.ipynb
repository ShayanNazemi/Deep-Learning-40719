{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "actor-critic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "96Sc-xN7OXqV"
      },
      "source": [
        "# CE-40719: Deep Learning\n",
        "## HW6 - Deep Reinforcement Learning\n",
        "(20 points)\n",
        "\n",
        "#### Name: Seyed Shayan Nazemi\n",
        "#### Student No.: 98209037"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-n6ALmbHO3Sb"
      },
      "source": [
        "In this assignment we are going to train a simple Actor-Critic model to solve classical control problems. We are going to use a batch version of the standard [gym](https://gym.openai.com/) library that is given to you in `multi_env.py`. The only difference between these two versions is that in `multi_env.py` instead of a single environment we have a batch of environments, therefore the observations are in shape `(batch_size * observation_size)`. We will focus on `CartPole-v1` problem but you can apply this to other problems as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jJSSI4_tlGAi"
      },
      "source": [
        "## Algorithm\n",
        "\n",
        "The vanilla actor-critic algorithm is as follows:\n",
        "\n",
        "1.   Sample a batch $\\{(s_i, a_i, r_i, s_{i + 1})\\}_i$ under policy $\\pi_\\theta$.\n",
        "2.   Fit $V_\\phi^{\\pi_\\theta}(s_i)$ to $r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})$ by minimizing squared error $\\|r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V_\\phi^{\\pi_\\theta}(s_i)\\|^2$.\n",
        "3. $\\max_{\\theta}~ \\sum_{i} \\log \\pi_\\theta(a_i|s_i) \\left[ r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V^{\\pi_\\theta}_\\phi(s_i) \\right]$\n",
        "\n",
        "We need two parametrized models, one for value function $V^{\\pi_\\theta}_\\phi$ and one for stochastic policy $\\pi_\\theta$. Since both $\\pi_\\theta$ and $V^{\\pi_\\theta}_\\phi$ are functions of state $s$, instead of modeling each with a seperate neural network, we can model both with a single network with shared parameters. In other words we train a single network that outputs both $\\pi_\\theta(a|s)$ and $V^{\\pi_\\theta}_\\phi(s)$. To train this network we combine step 2 and 3 in the main algoritm and optimize the following objective:\n",
        "$$\\min_{\\theta, \\phi}~ -\\sum_{i} \\log \\pi_\\theta(a_i|s_i) \\left[ r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V^{\\pi_\\theta}_\\phi(s_i) \\right] + \\|r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V_\\phi^{\\pi_\\theta}(s_i)\\|^2$$\n",
        "\n",
        "Note that the gradient must be backpropagated only through $\\log \\pi_\\theta(a_i|s_i)$ and $V_\\phi^{\\pi_\\theta}(s_i)$ in the squared error. A negative entropy term $-\\mathcal{H} (\\pi_\\theta(a_i|s_i))$ can also be added to above objective to encourage exploration. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dTSnFyiOyz7o"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_IYE3nBRhLWX",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as dist\n",
        "\n",
        "from multi_env import SubprocVecEnv"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hj9M77VaHsP7",
        "colab": {}
      },
      "source": [
        "env_name = 'CartPole-v1'\n",
        "num_envs = 16\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gym.make(env_name)\n",
        "        return env\n",
        "\n",
        "    return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pHy1yIj2CC31"
      },
      "source": [
        "## 1. Model (8 Points)\n",
        "\n",
        "To define a stochastic policy we use [`torch.distributions`](https://pytorch.org/docs/stable/distributions.html) module. Networks shared parameters are defined in a simple MLP. Network has two heads, one for $V$ that takes in MLPs output and outputs a scalar, and one for $\\pi$ that takes in the MLPs output and outputs a categorical distribution for each action. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L6aeF3ISTXK_",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_size, hidden_size, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        #################################################################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        # state_size: size of the input state\n",
        "        # hidden_size: a list containing size of each mlp hidden layer in order\n",
        "        # num_action: number of actions\n",
        "        # do not use batch norm for any layer in this network\n",
        "        #################################################################################\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size, bias=False)\n",
        "        self.fc2 = nn.Linear(hidden_size, 32, bias=False)\n",
        "\n",
        "        self.fc_policy = nn.Linear(32, num_actions, bias=False)\n",
        "        self.fc_value = nn.Linear(32, 1, bias=False)\n",
        "\n",
        "        pass\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #################################################################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        value = self.fc_value(x)\n",
        "        policy = dist.Categorical(F.softmax(self.fc_policy(x), dim=-1))\n",
        "        pass\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "        return policy, value"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0U1wUC0QhaT",
        "colab": {}
      },
      "source": [
        "def test_model(model):\n",
        "    env = gym.make(env_name)\n",
        "    total_reward = 0\n",
        "    #################################################################################\n",
        "    #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "    #################################################################################\n",
        "    # run given model for a single episode and compute total reward.\n",
        "    #################################################################################\n",
        "    done = False\n",
        "    obs = [torch.FloatTensor(env.reset()).to(device)] * num_state_obs\n",
        "\n",
        "    while done == False:\n",
        "        state = torch.cat(obs, dim=0).to(device).unsqueeze(0)\n",
        "\n",
        "        policy_dist, value = model(state)\n",
        "        action = policy_dist.sample()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(int(action.cpu().numpy()))\n",
        "        next_state = torch.FloatTensor(next_state).to(device)\n",
        "        obs = [*obs[1:], next_state]\n",
        "    \n",
        "        total_reward += reward\n",
        "    pass\n",
        "    #################################################################################\n",
        "    #                                   THE END                                     #\n",
        "    #################################################################################\n",
        "    return total_reward"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GP3XquD5zL-8"
      },
      "source": [
        "## 2. Objective and Training (12 Points)\n",
        "\n",
        "A single observation is not always enough to understand state of an environment, hence we take previous `num_state_obs` observations at time t as state of the environment at time t. Initialize and train the model using Adam optimizer. You should be able to get to 500 in less than 20000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IMhtTYWjz4X8",
        "colab": {}
      },
      "source": [
        "#################################################################################\n",
        "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "#################################################################################\n",
        "# experiment with different parameters and models to get the best result\n",
        "#################################################################################\n",
        "num_iterations = 40000\n",
        "num_state_obs = 10\n",
        "gamma = 0.99\n",
        "\n",
        "obs_size = 10\n",
        "state_size = num_state_obs * envs.observation_space.shape[0]\n",
        "num_actions = envs.action_space.n\n",
        "\n",
        "model = ActorCritic(state_size, 64, num_actions)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "#################################################################################\n",
        "#                                   THE END                                     #\n",
        "#################################################################################"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vArHT-zznKWL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4ca2dda3-4148-46e3-a445-75dd83f8acd3"
      },
      "source": [
        "obs = [torch.FloatTensor(envs.reset())] * num_state_obs\n",
        "for t in range(num_iterations):\n",
        "    model.train()\n",
        "    #################################################################################\n",
        "    #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "    #################################################################################\n",
        "    # implement the algorithm\n",
        "    #################################################################################\n",
        "    model.to(device)\n",
        "    \n",
        "    state = torch.cat(obs, dim=1).to(device)\n",
        "    policy_dist, value = model(state)\n",
        "    action = policy_dist.sample()\n",
        "\n",
        "    next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "    next_state = torch.FloatTensor(next_state)\n",
        "    \n",
        "    obs = [*obs[1:], next_state]\n",
        "\n",
        "    _, value_next = model(torch.cat(obs, dim=1).to(device))\n",
        "    done_mask = torch.tensor(1 - done, dtype=torch.float, device=device).unsqueeze(1)\n",
        "\n",
        "    reward = torch.FloatTensor(reward).to(device).unsqueeze(1)\n",
        "\n",
        "    Q_value = reward + done_mask * (gamma * value_next)\n",
        "    advantage = Q_value - value\n",
        "\n",
        "    loss = -torch.sum(policy_dist.log_prob(action).unsqueeze(0) * advantage.detach()) + F.mse_loss(Q_value.detach(), value, reduction='sum')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if t % 1000 == 999:\n",
        "        obs = [torch.FloatTensor(envs.reset())] * num_state_obs\n",
        "\n",
        "    pass\n",
        "    #################################################################################\n",
        "    #                                   THE END                                     #\n",
        "    #################################################################################\n",
        "    if t % 1000 == 999:\n",
        "        print('iteration {:5d}: average reward = {:5f}'.format(t + 1, np.mean([test_model(model) for _ in range(10)])))"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration  1000: average reward = 17.100000\n",
            "iteration  2000: average reward = 20.100000\n",
            "iteration  3000: average reward = 13.400000\n",
            "iteration  4000: average reward = 19.900000\n",
            "iteration  5000: average reward = 39.500000\n",
            "iteration  6000: average reward = 33.200000\n",
            "iteration  7000: average reward = 117.600000\n",
            "iteration  8000: average reward = 46.200000\n",
            "iteration  9000: average reward = 55.700000\n",
            "iteration 10000: average reward = 64.500000\n",
            "iteration 11000: average reward = 88.100000\n",
            "iteration 12000: average reward = 139.900000\n",
            "iteration 13000: average reward = 55.400000\n",
            "iteration 14000: average reward = 36.000000\n",
            "iteration 15000: average reward = 12.000000\n",
            "iteration 16000: average reward = 88.300000\n",
            "iteration 17000: average reward = 500.000000\n",
            "iteration 18000: average reward = 500.000000\n",
            "iteration 19000: average reward = 61.800000\n",
            "iteration 20000: average reward = 500.000000\n",
            "iteration 21000: average reward = 500.000000\n",
            "iteration 22000: average reward = 21.400000\n",
            "iteration 23000: average reward = 120.300000\n",
            "iteration 24000: average reward = 39.300000\n",
            "iteration 25000: average reward = 333.500000\n",
            "iteration 26000: average reward = 429.000000\n",
            "iteration 27000: average reward = 251.500000\n",
            "iteration 28000: average reward = 158.300000\n",
            "iteration 29000: average reward = 92.500000\n",
            "iteration 30000: average reward = 436.100000\n",
            "iteration 31000: average reward = 28.300000\n",
            "iteration 32000: average reward = 40.500000\n",
            "iteration 33000: average reward = 57.200000\n",
            "iteration 34000: average reward = 168.300000\n",
            "iteration 35000: average reward = 93.400000\n",
            "iteration 37000: average reward = 475.200000\n",
            "iteration 38000: average reward = 106.900000\n",
            "iteration 39000: average reward = 100.500000\n",
            "iteration 40000: average reward = 500.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ynU7aLnJKfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 245,
      "outputs": []
    }
  ]
}