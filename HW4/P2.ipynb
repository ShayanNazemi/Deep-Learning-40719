{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SpGEjxICnEV3"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "REDeZr0hSeCJ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# Assignment #4\n",
        "**CE4719: Deep Learing**\n",
        "\n",
        "\n",
        "*   Spring 2020\n",
        "*   http://ce.sharif.edu/courses/98-99/2/ce719-1/index.php\n",
        "\n",
        "**Please pay attention to these notes:**\n",
        "- the coding parts you have to complete are specified by:\n",
        "```\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    pass\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################ \n",
        "```\n",
        "- We always recommend discussion in groups for assignments. However, each student has to finish all of the questions by him/herself. \n",
        "- All submitted code will be compared against all student's codes using Stanford MOSS.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
        "- We HIGHLY encourage you to run this notebook on Google Colab.\n",
        "- **Before starting to work on the assignment, please fill your name in the next section AND Remember to RUN the cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "mk4Bs7vnsfvG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72e6fe66-408b-4fab-a646-770f8a485efb"
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\"\n",
        "student_id = \"98209037\" #@param {type:\"string\"}\n",
        "student_name = \"Shayan Nazemi\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your student id: 98209037\n",
            "your name: Shayan Nazemi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "XFWqU5noQjQC",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## 2. Tokenization, Vocabulary, Preprocessing (15 pts)\n",
        "\n",
        "---\n",
        "In this problem, you will practice tokenization, creating vocabulary for a corpus, preprocessing data, and processing data using RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "hu4Yi7q8XwIX",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 2.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "sxtV0x6rXsNJ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0c27f8cd-86ac-4084-de75-7711955632a6"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn import RNN, RNNCell, Embedding\n",
        "from typing import List, Dict\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "# nltk will be used to tokenize texts\n",
        "import nltk  \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "tn5IAVDGhG-f",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdeeeb19-fffc-4ac6-f7ef-447ab6c2fc83"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "# Function for setting the random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if DEVICE == torch.device(\"cuda\"):\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.cuda.empty_cache()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "z1yhrX8gXy-E",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 2.2 Tokenize (1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "ZJEQ9xt6P8AY",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Tokenization is the process of tokenizing or splitting a string or text into a list of tokens. Tokenization is one of the common pre-processings in natural language processing. The resulting tokens are then passed on to some other form of processing, which in our case will be deep neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "NaD8qSkxXuiW",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "def tokenize(corpus: List[str]) -> List[List[str]]:\n",
        "    \"\"\" tokenizes the corpus and returns it as a list of list of tokens\n",
        "    corpus: Input corpus as a list of sentences (each sentence is a string)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: use nltk word_tokenize to tokenize corpus, as a list of sentences,     #\n",
        "    # into its constituent words.                                                  #\n",
        "    # You should first lowercase the characters of sentences (use .lower()         #\n",
        "    # methods of strings.)                                                         #\n",
        "    ################################################################################\n",
        "    tokens = []\n",
        "    for sentence in corpus:\n",
        "        tokens.append(nltk.word_tokenize(sentence.lower()))\n",
        "\n",
        "    return tokens\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "OqYxSgEdbgOd",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's test your implementation with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "zl31abucSYR5",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b96781db-c1d9-4a57-a479-b39430e05b36"
      },
      "source": [
        "def test_tokenize():\n",
        "    sample_corpus = [\"From fairest creatures we desire increase.\",\n",
        "                     \"Within thine own bud buriest thy content.\",\n",
        "                     \"Thy youth's proud livery so gazed on now.\", \n",
        "                     \"Shall sum my count, and make my old excuse.\"]\n",
        "\n",
        "    correct_answer = [['from', 'fairest', 'creatures', 'we', 'desire', 'increase', '.'],\n",
        "                      ['within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', '.'],\n",
        "                      ['thy', 'youth', \"'s\", 'proud', 'livery', 'so', 'gazed', 'on', 'now', '.'],\n",
        "                      ['shall', 'sum', 'my', 'count', ',', 'and', 'make', 'my', 'old', 'excuse', '.']]\n",
        "\n",
        "    assert tokenize(sample_corpus) == correct_answer\n",
        "\n",
        "    print('passed!')\n",
        "\n",
        "test_tokenize()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "n7MdJpsagYpK",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 2.3 Vocabulary (6 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "hhiS8HnNQfmB",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "After tokenizing the corpus, we will find the unique tokens and name them the vocabulary of that corpus. There are a couple of important points to note here:\n",
        "\n",
        "1. When dealing with a corpus in sentence-wise manner, we usually mark the beginning and end of sentences with some special tokens (e.g., `'<START>'` and `'<END>'`).\n",
        "\n",
        "2. To make sequences have the same length, we pad shorter ones with a special token (e.g., `'<PAD>'`).\n",
        "\n",
        "3. Tokens which will be encountered later and do not exist in our vocab will be replaced by a special token (e.g., `'<UNK>'`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "XCvF_KrVX294",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, word2id=None):\n",
        "        \"\"\"Constructor of Vocab\n",
        "\n",
        "        word2id: dictionary that maps tokens to their ids.\n",
        "        \"\"\"\n",
        "        self.pad_token = '<PAD>'\n",
        "        self.end_token = '<END>'\n",
        "        self.start_token = '<START>'\n",
        "        self.unk_token = '<UNK>'\n",
        "\n",
        "        if word2id is None:\n",
        "            self.word2id = {self.pad_token: 0,\n",
        "                            self.start_token: 1,\n",
        "                            self.end_token: 2,\n",
        "                            self.unk_token: 3}\n",
        "            self.size = 4\n",
        "        else:\n",
        "            self.word2id = word2id\n",
        "            self.size = len(self.word2id)\n",
        "\n",
        "        self.id2word = {v: k for (k, v) in self.word2id.items()}\n",
        "\n",
        "    def build(self, tokenized_corpus: List[List[str]], size=None, min_freq=None):\n",
        "        \"\"\"Builds the vocab from a tokenized corpus.\n",
        "\n",
        "        tokenized_corpus: corpus as a list of list of tokens (strings)\n",
        "        size: Final size of (number of unique tokens in) our vocab\n",
        "        min_freq: minimum frequency\n",
        "        \"\"\"\n",
        "        tokens2freq = Counter(chain(*tokenized_corpus))  # dict that maps unique tokens to their freqs in the corpus\n",
        "        frequent_tokens = []\n",
        "        ################################################################################\n",
        "        # TODO: use tokens2freq and find the first size frequent tokens and save       #\n",
        "        #       them in frequent_tokens. Remove tokens with a frequency lower than     #\n",
        "        #       min_freq (i.e. if token's occurence in the corpus is less than         # \n",
        "        #       min_freq times, don't put the token in frequent_tokens).               #\n",
        "        #       If size is None, then use all of the tokens. This also applies to      #\n",
        "        #       min_freq.                                                              #\n",
        "        ################################################################################\n",
        "        tokens2freq = {key: value for key, value in sorted(tokens2freq.items(), key=lambda item: item[1])}\n",
        "        if size :\n",
        "            tokens2freq = dict(list(tokens2freq.items())[:size])\n",
        "        \n",
        "        if min_freq : \n",
        "            tokens2freq = dict(list(filter(lambda i : i[1] >= min_freq, tokens2freq.items())))\n",
        "\n",
        "        frequent_tokens = [k for k,v in tokens2freq.items()]\n",
        "\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        # adding tokens to the vocab\n",
        "\n",
        "        for token in frequent_tokens:\n",
        "            self.add_token(token)\n",
        "\n",
        "    def get_token_by_id(self, t_id: int) -> str:\n",
        "        \"\"\"Returns the token with the corresponding id in the vocab.\n",
        "        If the id is not valid, returns None.\n",
        "\n",
        "        t_id: token id\n",
        "        \"\"\"\n",
        "        return self.id2word.get(t_id, None)\n",
        "\n",
        "    def get_id_by_token(self, token: str) -> int:\n",
        "        \"\"\"Returns the id of the token in the vocab. If the token does not exist,\n",
        "        returns the id of <UNK> token.\n",
        "\n",
        "        token: token (as a string) for which the id should be returned.\n",
        "        \"\"\"\n",
        "        return self.word2id.get(token, self.word2id[self.unk_token])\n",
        "\n",
        "    def add_token(self, token: str):\n",
        "        \"\"\"Adds the token to the vocab's data structures\n",
        "        token: token as a string\n",
        "        \"\"\"\n",
        "        ################################################################################\n",
        "        # TODO: If the token is not already in the vocab add it to word2id and id2word #\n",
        "        # Don't forget to update the vocab size afterwards!                            #\n",
        "        ################################################################################\n",
        "        try :\n",
        "            self.word2id[token]\n",
        "        except:\n",
        "            L = self.size\n",
        "            self.word2id[token] = L\n",
        "            self.id2word[L] = token\n",
        "        \n",
        "        self.size += 1\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def tokens2ids(self, sents):\n",
        "        \"\"\"Convert list of words or list of sentences of tokens \n",
        "        into list or list of list of indices.\n",
        "\n",
        "        sents: input sentences as List[List[str]] (multiple sentences) or List[str]\n",
        "        (single sentence)\n",
        "        \"\"\"\n",
        "        ################################################################################\n",
        "        # TODO: return a new list where each token is repalced by its id.              #\n",
        "        # HINT: try to implement each part in one line of code using list comprehension#\n",
        "        ################################################################################\n",
        "        if type(sents[0]) == list:\n",
        "            return [[self.word2id[token] if token in self.word2id else self.word2id[self.unk_token] for token in sentence] for sentence in sents]\n",
        "        else:\n",
        "            return [self.word2id[token] if token in self.word2id else self.word2id[self.unk_token] for token in sents]\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "  \n",
        "    def to_tensor(self, sent: List[str]):\n",
        "        \"\"\"Converts a sentence as a list of tokens into a tensor of indices.\n",
        "\n",
        "        sent: a sentence as a list of strings (tokens)\n",
        "        \"\"\"\n",
        "        ################################################################################\n",
        "        # TODO: Use self.tokens2ids to get the sentence as a list of indices and wrap a#\n",
        "        #       tensor around it with dtpye of torch.long and device of DEVICE.        #\n",
        "        ################################################################################\n",
        "        ids = self.tokens2ids(sent)\n",
        "        return torch.tensor(ids, dtype=torch.long, device=DEVICE)\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def pad_sents(self, sents: List[List[str]]) -> List[List[str]]:\n",
        "        \"\"\"Pads list of sentences according to the longest sentence.\n",
        "\n",
        "        sents: sentences as a list of list of tokens (strings).\n",
        "        \"\"\"\n",
        "        sents_padded = []\n",
        "        ################################################################################\n",
        "        # TODO: pad shorter sentences by appending pad token to them.                  #\n",
        "        ################################################################################\n",
        "        sents_padded = sents\n",
        "        longest = len(max(sents, key= lambda sentence : len(sentence)))\n",
        "        \n",
        "        for sentence in sents_padded:\n",
        "            diff = longest - len(sentence)\n",
        "            sentence += [self.pad_token] * diff\n",
        "\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        return sents_padded\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Saves the vocab in a json file.\n",
        "\n",
        "        path: path to save the vocab in\n",
        "        \"\"\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.word2id, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path: str):\n",
        "        \"\"\"Loads vocab from a json file.\n",
        "\n",
        "        path: path to load the vocab from\n",
        "        \"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            word2id = json.load(f)\n",
        "\n",
        "        return Vocab(word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "1V5TDv5NlCbV",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Implement `add_token` and `build` methods. Then, test your implementation with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "UYPEzxgsks5z",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3dfa4863-be1b-4cfe-a366-2a7bb3d7194d"
      },
      "source": [
        "def test_build_vocab():\n",
        "    vocab =  sample_corpus = [\"From fairest creatures we desire increase.\",\n",
        "                              \"Within thine own bud buriest thy content.\",\n",
        "                              \"Thy youth's proud livery so gazed on now.\", \n",
        "                              \"Shall sum my count, and make my old excuse.\"]\n",
        "\n",
        "    vocab = Vocab()\n",
        "    vocab.build(tokenize(sample_corpus))\n",
        "\n",
        "    assert set(vocab.word2id.keys()) == {'old', '<UNK>', 'on', '<END>', 'gazed', 'now',\n",
        "                                         'own', 'my', 'proud', 'content', ',', 'desire',\n",
        "                                         'and', '<PAD>', 'shall', 'from', 'creatures',\n",
        "                                         'sum', 'fairest', 'youth', \"'s\", 'make', 'increase',\n",
        "                                         'excuse', 'we', 'bud', 'thine', '.', '<START>',\n",
        "                                         'livery', 'thy', 'count', 'buriest', 'within', 'so'}\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab = test_build_vocab()\n",
        "print('passed!')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "lQLJJ0zloJYD",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Implement `pad_sents` method and test it with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "coszZZJmoIC4",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d8c7ac1-a6ac-4e7d-cb65-d5f13f9c0e42"
      },
      "source": [
        "def test_pad():\n",
        "    vocab = test_build_vocab()\n",
        "\n",
        "    unpadded_sents = [['to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes'], \n",
        "                      ['of', 'his', 'self-love', 'to', 'stop', 'posterity',], \n",
        "                      ['die', 'single'],\n",
        "                      ['then', 'beauteous', 'niggard']]\n",
        "\n",
        "    padded_sents = vocab.pad_sents(unpadded_sents)\n",
        "\n",
        "    assert padded_sents == [['to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes'],\n",
        "                            ['of', 'his', 'self-love', 'to', 'stop', 'posterity', '<PAD>', '<PAD>'],\n",
        "                            ['die', 'single', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'],\n",
        "                            ['then', 'beauteous', 'niggard', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n",
        "    print('passed!')\n",
        "\n",
        "test_pad()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "6XwEHP9sSw2o",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Implement `tokens2ids` and check your output with the following function (check whether each token is replaced by a correct id):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "e1JGewypSt26",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "4a512134-c204-4119-f78f-2b5f0ace1abf"
      },
      "source": [
        "def test_tokens2id():\n",
        "    vocab = test_build_vocab()\n",
        "    sents = [['to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes'],\n",
        "             ['increase', 'his', 'we', 'to', 'stop', 'desire', '<PAD>', '<PAD>'],\n",
        "             ['die', 'single', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'],\n",
        "             ['then', 'proud', 'livery', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n",
        "\n",
        "    sent = ['increase', 'proud', 'stop', 'sunken', '<PAD>']\n",
        "\n",
        "    pprint(vocab.word2id)\n",
        "    print()\n",
        "    pprint(vocab.tokens2ids(sents))\n",
        "    print()\n",
        "    pprint(vocab.tokens2ids(sent))\n",
        "\n",
        "\n",
        "test_tokens2id()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"'s\": 17,\n",
            " ',': 27,\n",
            " '.': 34,\n",
            " '<END>': 2,\n",
            " '<PAD>': 0,\n",
            " '<START>': 1,\n",
            " '<UNK>': 3,\n",
            " 'and': 28,\n",
            " 'bud': 13,\n",
            " 'buriest': 14,\n",
            " 'content': 15,\n",
            " 'count': 26,\n",
            " 'creatures': 6,\n",
            " 'desire': 8,\n",
            " 'excuse': 31,\n",
            " 'fairest': 5,\n",
            " 'from': 4,\n",
            " 'gazed': 21,\n",
            " 'increase': 9,\n",
            " 'livery': 19,\n",
            " 'make': 29,\n",
            " 'my': 33,\n",
            " 'now': 23,\n",
            " 'old': 30,\n",
            " 'on': 22,\n",
            " 'own': 12,\n",
            " 'proud': 18,\n",
            " 'shall': 24,\n",
            " 'so': 20,\n",
            " 'sum': 25,\n",
            " 'thine': 11,\n",
            " 'thy': 32,\n",
            " 'we': 7,\n",
            " 'within': 10,\n",
            " 'youth': 16}\n",
            "\n",
            "[[3, 3, 10, 11, 12, 3, 3, 3],\n",
            " [9, 3, 7, 3, 3, 8, 0, 0],\n",
            " [3, 3, 0, 0, 0, 0, 0, 0],\n",
            " [3, 18, 19, 0, 0, 0, 0, 0]]\n",
            "\n",
            "[9, 18, 3, 3, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "OfR1rtMPVbaG",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Implement `to_tensor()` method and test it with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "mdMApxnvVZS_",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcb040ac-7c13-4073-81ce-da1f52ae3283"
      },
      "source": [
        "def test_to_tensor():\n",
        "    sent = ['increase', 'proud', 'stop', 'sunken', '<PAD>']\n",
        "    vocab = test_build_vocab()\n",
        "\n",
        "    out = vocab.to_tensor(sent)\n",
        "\n",
        "    assert torch.is_tensor(out)\n",
        "    print(out)\n",
        "\n",
        "test_to_tensor()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 9, 18,  3,  3,  0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "E9-AdlfHgjVy",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 2.4 Pad & Pack and using them with RNNs in PyTorch (8 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "2zzAQos1gs_K",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "You might want to process a batch of data before and after feeding to a RNN (instead of directly feeding it in and using its output). In this section, we will look at some of the ways of doing this in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "AW7ATZE8huk9",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 2.4.1 `pad_sequence()` (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "uZ6VqPdt3Qmw",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "`pad_sequence` is used to convert variable length sequences to same size. You can do padding either manually (like what we you did in the previous part) or by using `torch.nn.utils.rnn.pad_sequence()`. <br/>\n",
        "Now let's use this function to pad some data and then feed it to a RNN. With your knowledge of PyTorch at this point, you should be able to read [torch.nn documentation](https://pytorch.org/docs/stable/nn.html) and figure out how to do this task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "OsNHjjeih9HI",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def pad_and_feed(data: List[torch.Tensor]) -> torch.Tensor: \n",
        "    \"\"\"Pads shorter sentences in a list of tensors and feeds it to a RNN.\n",
        "\n",
        "    data: data as a list of tensors with torch.int dtype.\n",
        "    \"\"\" \n",
        "    rnn, output = None, None\n",
        "    ################################################################################\n",
        "    # TODO: 1) pads the data using pad_sequence of PyTorch                         #\n",
        "    #       2) Instantiate rnn using torch.nn.RNN with appropriate input size and  #\n",
        "    #       hidden size of 10.                                                     #\n",
        "    #       save the outputs (hidden states) of applying the rnn to the data in    #\n",
        "    #       the output variable.                                                   #         \n",
        "    ################################################################################\n",
        "    padded = pad_sequence(data)\n",
        "    rnn = torch.nn.RNN(padded.size(-1), hidden_size = 10)\n",
        "    (output, _) = rnn(padded)\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################ \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "wrwonn07u3Jr",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's test your implementation with the following function (the test is not exhaustive and is just checking shapes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "e2OidlVek5FJ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f06de24c-9d68-4745-80d3-2e2eee2766d2"
      },
      "source": [
        "def test_pad_and_feed():\n",
        "    np.random.seed(42)\n",
        "    data = [torch.empty(np.random.randint(0, 10), 100) for _ in range(5)]\n",
        "    assert pad_and_feed(data).shape == torch.Size([7, 5, 10])\n",
        "    print('passed!')\n",
        "\n",
        "test_pad_and_feed()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "SpGEjxICnEV3",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 2.4.2 `pack_sequence()` (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "Rkicrz0Y5Vy_",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "After padding the data, there will be a lot of zeros (we often set the id of pad token to zero) representing pads in it. These zeros do not really need to be processed by RNN because they do not represent any meaningful data! In fact, we just use them to make a single tensor from our variable length sentences. Therefore, feeding padded data directly to RNN is inefficient.\n",
        "\n",
        "In order to make the process more efficient, there is a function called `pack_sequence()` that reforms the data so that the model can just process useful tokens and not the paddings.\n",
        "\n",
        "Let's see what exactly pack does:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "ZejMlf2Twbym",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "87afe77a-c606-4c3a-ee09-258eda8e1a65"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_sequence\n",
        "\n",
        "def pack():\n",
        "    a = torch.zeros((5, 8))\n",
        "    b = torch.ones((3, 8))\n",
        "    c = 2 * torch.ones((1, 8))\n",
        "    ################################################################################\n",
        "    # TODO:  pack the above three tensors and print the packed output.             #\n",
        "    ################################################################################\n",
        "    output = pack_sequence([a, b, c])\n",
        "    return output\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################ \n",
        "\n",
        "pack()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]]), batch_sizes=tensor([3, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "yi7IBIFzEJs6",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "##### **Questions**: \n",
        "\n",
        "1) what pattern do you see in the packed data? \n",
        "\n",
        "**YOUR ANSWER**: <<< Write your answer here >>>\n",
        "\n",
        "2) what does `batch_sizes` mean in the packed data? <br/>\n",
        "\n",
        "**YOUR ANSWER**: <<< Write your answer here >>>\n",
        "\n",
        "3) How do you think RNN processes packed data?\n",
        "\n",
        "**YOUR ANSWER**: <<< Write your answer here >>>\n",
        "\n",
        "Feel free to write your answers in Persian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "TuFVHmdYmNaE",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 2.4.3 `pack_padded_sequence()` and `pad_packed_sequence()` (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "30tezAidG_lM",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "You will usually see that two functions are frequenlty used to process data before and after feeding it to the RNNs: `pack_padded_sequence()` and `pad_packed_sequence()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "QCc6Q7ycj_er",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "# We will import these functions as pack and unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "_U90CFyAOtMd",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def pad_and_pack(data: List[torch.Tensor]) -> torch.Tensor:\n",
        "    data = sorted(data, key=lambda element: element.shape[0], reverse=True)\n",
        "    lengths = [d.shape[0] for d in data]\n",
        "    ################################################################################\n",
        "    # TODO: 1) pad the data using pad_sequence()                                   #\n",
        "    #       2) pack the data and feed it to a RNN with hidden size of 10           #\n",
        "    #       3) unpack the RNN's outputs (hidden states) and return it              #\n",
        "    ################################################################################   \n",
        "    padded = pad_sequence(data)\n",
        "    packed = pack(padded, lengths)\n",
        "\n",
        "\n",
        "    rnn = RNN(padded.size(-1), 10)\n",
        "    output, h = rnn(packed)\n",
        "    seq_unpacked, lengths_ = unpack(output)\n",
        "    return seq_unpacked\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "0qFG6Qdpzfo3",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's test your implementation with the following function (the test is not exhaustive and is just checking shapes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "VnyWoSa3zkcy",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f21bf9af-b86c-4813-900f-b8ab3f06e9aa"
      },
      "source": [
        "def test_pad_and_pack():\n",
        "    np.random.seed(42)\n",
        "    data = [torch.empty(np.random.randint(0, 10), 100) for _ in range(5)]\n",
        "    outputs = pad_and_pack(data)\n",
        "    assert outputs.size() == torch.Size([7, 5, 10])\n",
        "    print('passed!')\n",
        "\n",
        "test_pad_and_pack()"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "7rEzSkHNOGo4",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 2.5 Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "42IqDbUEOFsB",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "As you have seen in previous parts, the shape of a batch of our sentences would be $(N, L)$ where $N$ is the number of sentences and $L$ is the length of the longest sequence (Note that after padding all of the sequences have the same length $L$). Each entry of this tensor is the id of a token in our vocab. <br/>\n",
        "\n",
        "Feeding these integer ids directly to RNNs is obviously a bad idea. We need to represent each token of our vocabulary with a dense vector. That's where embeddings comes in. <br/>\n",
        "\n",
        "You can instantiate embedding layers in PyTorch by using `torch.nn.Embedding`. If you feed a Tensor of vocab indices with shape $(d_1, d_2, ..., d_k)$ to an embedding layer, it will return a $(d_1, d_2, ..., d_k, D)$ tensor where $D$ is the embedding dimension of the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "sJAsaEK1sfwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a9d4432d-7d17-48c2-f359-9dc4520f9a70"
      },
      "source": [
        "def embed():\n",
        "    np.random.seed(40719)\n",
        "    data = torch.tensor([[1, 2, 4, 4], [1, 3, 0, 0], [2, 0, 0, 0]])\n",
        "    layer = nn.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=0)  # what is padding_idx?\n",
        "    with torch.no_grad():\n",
        "        print(layer(data))\n",
        "\n",
        "embed() "
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.9482, -1.2454,  0.2676],\n",
            "         [ 0.9063,  0.2011,  1.5885],\n",
            "         [ 0.2238,  1.2634,  1.4606],\n",
            "         [ 0.2238,  1.2634,  1.4606]],\n",
            "\n",
            "        [[ 1.9482, -1.2454,  0.2676],\n",
            "         [-1.1144,  0.8153,  0.7975],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.9063,  0.2011,  1.5885],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "kOF6ybSFOMVt",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 2.6 Putting it all together (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "bx8RH--bzx0n",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's put all the stuff we learned above together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "QVsaGw5Pm5G3",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def process_data(data: List[str], vocab: Vocab) -> torch.Tensor:\n",
        "    outputs = None\n",
        "    ################################################################################\n",
        "    # TODO: 1) tokenize the data using your tokenize function.                     #                                     \n",
        "    #       2) use vocab's pad_sents method to pad the data.                       #\n",
        "    #       3) replace tokens with their ids in the vocab.                         #\n",
        "    #       4) instantiate an embedding layer with embedding size of 10 and        #\n",
        "    #          vocabulary size equal to vocab.size.                                #\n",
        "    #       5) wrap a tensor around the data and apply the embedding layer to it.  # \n",
        "    #       6) pack the data and feed it to a RNN with hidden size of 5.           #\n",
        "    #       7) unpack the RNN's outputs (hidden states) and save it in outputs.    #\n",
        "    ################################################################################ \n",
        "    tokenized = tokenize(data)\n",
        "    padded = vocab.pad_sents(tokenized)\n",
        "    ids = vocab.tokens2ids(padded)\n",
        "\n",
        "    layer = nn.Embedding(num_embeddings=vocab.size, embedding_dim=10, padding_idx=0)\n",
        "    embeded = layer(torch.tensor(ids))\n",
        "\n",
        "    packed = pack_sequence(embeded)\n",
        "\n",
        "    rnn = RNN(packed.data.size(-1), 5)\n",
        "    output, h = rnn(packed)\n",
        "    outputs, lengths_ = unpack(output)\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "id": "5leRPmPB4VwR",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's test your implementation (the test is not exhaustive and is just checking shapes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "id": "1TNt4plR4YZm",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14cdd390-4956-44b9-9e5b-03fa5b5c8f58"
      },
      "source": [
        "def test_process_data():\n",
        "    vocab = test_build_vocab()\n",
        "    sents = ['to say within thine own deep sunken eyes', \n",
        "             'of his self-love to stop posterity', \n",
        "             'then beauteous niggard'\n",
        "             'die single']\n",
        "\n",
        "    outputs = process_data(sents, vocab)\n",
        "    assert outputs.shape == torch.Size([8, 3, 5])\n",
        "    print('passed!')\n",
        "\n",
        "test_process_data()"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG0_Hr56pJj8",
        "colab_type": "text"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3r5rMBbqNpz",
        "colab_type": "text"
      },
      "source": [
        "- Check and review your answers. Make sure all cells' output are what you have planned.\n",
        "- Select File > Save.\n",
        "- To download the notebook, select File > Download .ipynb.\n",
        "- Create an archive of all notebooks (P1.ipynb, P2.ipynb, and P3.ipynb)"
      ]
    }
  ]
}